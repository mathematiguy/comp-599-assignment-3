{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71415c4f-87bf-4697-94f6-e551d020893b",
   "metadata": {},
   "source": [
    "### Compare the ShallowBiLSTM class with the regular unidirectional LSTM class provided (5 pts)\n",
    "\n",
    "Note: the training for these two models may take a while.\n",
    "\n",
    "Load in the SNLI dataset using Pytorch’s DataLoader class. Use `batch_size=32` and\n",
    "`shuffle=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40cec4fc-4931-4a3b-84c4-2019252a6dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from code import *\n",
    "\n",
    "batch_size=32\n",
    "num_layers=1\n",
    "num_classes=3\n",
    "hidden_dim=512\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ec6fb54-9374-4484-bb80-8a163658e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(emb_dict, emb_dim), (dataloader_train, dataloader_valid, dataloader_test) = load_snli_dataset(batch_size)\n",
    "\n",
    "word_counts = build_word_counts(dataloader_train)\n",
    "index_map = build_index_map(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b2e746-bdaf-4c01-9eb3-e2e868aacae9",
   "metadata": {},
   "source": [
    "## Initialise the model with Glove embeddings\n",
    "\n",
    "Use `create_embedding_matrix` to create the embedding matrix initialized with Glove embeddings that you will use to initialize your `embedding_layer`. Use `from_pretrained` for this purpose (consult PyTorch `nn.Embedding` documentation if you need to).\n",
    "\n",
    "Overwrite the embedding layer of your model (outside the class, in the training code) after initializing it. Pass `freeze=False` and `padding_idx=0`.\n",
    "\n",
    "We do not want the Glove embeddings to be frozen, as having them trainable can only boost our performance. Due to our `embedding_size == hidden_dim` simplification, you need to modify your classes in the following way:\n",
    "\n",
    "- Decouple the `hidden_dim` from the `embedding_size` in your model, by passing another parameter (e.g. `embedding_size`) to your `model.__init__` method and setting it separately, to initialize your Embedding layer within `__init__`. This will require you to tweak the input features to the `LSTM`(s) as well.\n",
    "- Make sure any tensors you make are moved to the GPU with `.to(device)` where `device` is `torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")`\n",
    "- Use `evaluate` method to evaluate dataset performance on validation and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c4384c-b725-4025-970d-5887e3a624f9",
   "metadata": {},
   "source": [
    "To make a fair comparison between the two models, halve the `hidden_dim` parameter on the `BiLSTM` for your comparison \n",
    "(e.g. 100-dim for `UniLSTM`, 50-dim for `ShallowBiLSTM`, see note about Glove embeddings above)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3441cd75-e8ba-4b2c-bde8-3b521557ddd2",
   "metadata": {},
   "source": [
    "Modify your `UniLSTM` (or alternatively your `ShallowBiLSTM`) by passing `bidirectional=True` and `num_layers=2` with a single `LSTM` initialization (modifying the incoming feature size to the Linear layers accordingly) to create a true `BiLSTM` to compare against. Compare this model against the equivalent `UniLSTM` and `ShallowBiLSTM`.\n",
    "- You will need to modify the concatenation as well, to concatenate 4 different cell states instead of just 2. The bidirectional `LSTM` with `num_layers=2` will output a tensor of `[4, BATCH_SIZE, hidden_size]` for the final cell state.\n",
    "- In this case, `final_cell_state[-1,:,:]` and `final_cell_state[-2,:,:]` are the now 2 outputs (forward and backward LSTMs) that we care about, similar to `ShallowBiLSTM`. The difference is that now we have a deeper network, with interconnections between layers – PyTorch takes care of this for us (true `BiLSTM` vs. shallow `BiLSTM`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6414b0-c5ad-47e3-bbb1-3ec648b89767",
   "metadata": {},
   "source": [
    "Start with the `num_layers` as 1. Feel free to play around with increasing it, but make sure that you’re making a fair comparison always (approx same number of params overall). \n",
    "\n",
    "Increase it to beyond 1, making sure you’re taking the cell state from the final layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5a353d-c603-4a31-8017-2a3b8d530126",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset snli (/home/user/.cache/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b)\n",
      "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 840.88it/s]\n",
      "Loading cached processed dataset at /home/user/.cache/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-d1fc920dc7ec80e4.arrow\n",
      "Loading cached processed dataset at /home/user/.cache/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-828f37787a2fb016.arrow\n",
      "Loading cached processed dataset at /home/user/.cache/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-7a32c32b24afac81.arrow\n",
      "Loading cached processed dataset at /home/user/.cache/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-31606576c379c50e.arrow\n",
      "  0%|                                                   | 0/100 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "progress = {}\n",
    "\n",
    "progress[\"unilstm\"] = run_snli(\n",
    "    UniLSTM,\n",
    "    num_epochs=num_epochs,\n",
    "    vocab_size=len(index_map),\n",
    "    hidden_dim=hidden_dim,\n",
    "    embedding_size=emb_dim,\n",
    "    num_layers=num_layers,\n",
    "    num_classes=num_classes,\n",
    "    use_glove=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20b31c4-f153-462a-a3b2-6d077e977087",
   "metadata": {},
   "outputs": [],
   "source": [
    "progress[\"unilstm_glove\"] = run_snli(\n",
    "    UniLSTM,\n",
    "    num_epochs=num_epochs,\n",
    "    vocab_size=len(index_map),\n",
    "    hidden_dim=hidden_dim,\n",
    "    embedding_size=emb_dim,\n",
    "    num_layers=num_layers,\n",
    "    num_classes=num_classes,\n",
    "    use_glove=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086833ab-6a28-4321-9a14-0c7b1bdc7589",
   "metadata": {},
   "outputs": [],
   "source": [
    "progress[\"shallow_bilstm\"] = run_snli(\n",
    "    ShallowBiLSTM,\n",
    "    num_epochs=num_epochs,\n",
    "    vocab_size=len(index_map),\n",
    "    hidden_dim=hidden_dim,\n",
    "    embedding_size=emb_dim,\n",
    "    num_layers=num_layers,\n",
    "    num_classes=num_classes,\n",
    "    use_glove=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f193ed-aeeb-4b97-908a-1f3129071a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "progress[\"shallow_bilstm_glove\"] = run_snli(\n",
    "    ShallowBiLSTM,\n",
    "    num_epochs=num_epochs,\n",
    "    vocab_size=len(index_map),\n",
    "    hidden_dim=hidden_dim,\n",
    "    embedding_size=emb_dim,\n",
    "    num_layers=num_layers,\n",
    "    num_classes=num_classes,\n",
    "    use_glove=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2516540c-47e7-4a8a-a606-a4d481c73cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "progress[\"bilstm\"] = run_snli(\n",
    "    BiLSTM,\n",
    "    num_epochs=num_epochs,\n",
    "    vocab_size=len(index_map),\n",
    "    hidden_dim=hidden_dim,\n",
    "    embedding_size=emb_dim,\n",
    "    num_layers=num_layers,\n",
    "    num_classes=num_classes,\n",
    "    use_glove=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7beac1-b0e6-470a-80e8-e08777bdaa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "progress[\"bilstm_glove\"] = run_snli(\n",
    "    BiLSTM,\n",
    "    num_epochs=num_epochs,\n",
    "    vocab_size=len(index_map),\n",
    "    hidden_dim=hidden_dim,\n",
    "    embedding_size=emb_dim,\n",
    "    num_layers=num_layers,\n",
    "    num_classes=num_classes,\n",
    "    use_glove=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c6eb9b-1e47-44ea-a30c-78e91ac71d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data/snli_progress.json', 'w') as f:\n",
    "    json.dump(progress, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26d7630-a3ca-41dc-89ec-d483a59122f4",
   "metadata": {},
   "source": [
    "Compare 2 versions of each model: 1 with Glove embeddings and 1 without.\n",
    "- What do you observe in these two cases?\n",
    "- What is the difference in performance between the two models in the two cases for each? \n",
    "- Does this correspond to what you expected? Provide a potential explanation for what you observe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b2d33d-9790-4245-bbf8-6ae7b4e0d08d",
   "metadata": {},
   "source": [
    "- Report final accuracy on the validation and test set for each model (of the same epoch), with and without Glove embeddings.\n",
    "- Comment on the efficacy of the Glove embeddings.\n",
    "- Plot accuracy over time on the validation and test set for each model, with and without Glove embeddings.\n",
    "- Train the model until validation accuracy stops going up.\n",
    "- Do you observe any other differences between the two models? Training time?\n",
    "- You should have 5 different configurations being compared in the end. UniLSTM with Glove, UniLSTM without Glove, ShallowBiLSTM with Glove, ShallowBiLSTM without Glove, True BiLSTM (modified UniLSTM)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
