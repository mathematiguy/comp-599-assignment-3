{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf51e326-8c35-455d-98df-d7b33ae99f4e",
   "metadata": {},
   "source": [
    "# Comp 599 - Assignment 3 report\n",
    "\n",
    "This notebook completes the report section of Comp 599 Natural Language Understanding - Assignment 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56e87e28-4c3f-4c27-a1cc-432b0ce45763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from code import *\n",
    "\n",
    "progress = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00c151e-0a7d-4896-aa1c-762cc66dc7b7",
   "metadata": {},
   "source": [
    "## Problems\n",
    "\n",
    "### 1.3 Training (not autograded, needed for REPORT) (10 pts)\n",
    "Some of the training code is provided. Fill in the blanks, specifically converting the output of our `sample_sequence` method back to natural language and printing it.\n",
    "\n",
    "**Notes:**\n",
    "- Make sure any tensors you make are moved to the GPU with .to(device) where device is “torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")”\n",
    "- Make sure you are passing a new hidden state for each training iteration, passing the same one will cause computation graph errors with Pytorch as it tries to accumulate the losses from all previous batches. The main loop of your training code should do the following, in this sequence:\n",
    "1. Apply the RNN to the incoming sequence\n",
    "2. Use the loss function to calculate the loss on the model’s output\n",
    "3. Zero the gradients of the optimizer\n",
    "4. Perform a backward pass (calling `.backward()`)\n",
    "5. Step the weights of the model via the optimizer (`.step()`)\n",
    "6. Add the current loss to the running loss\n",
    "\n",
    "**After every epoch, you should:**\n",
    "1. Sample a character with some randomness (this is up to you, you could sample from the dataset or you could sample uniformly from unique characters, or only capital letters, or any other variation).\n",
    "2. Print the result of the sampling to the output, so you can monitor the process of the training. Over time your generated text should become more and more “believable”. If not, something may be wrong with your implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c451282f-b566-4ab1-8c72-0c435175f565",
   "metadata": {},
   "source": [
    "**For the REPORT, please do the following:**\n",
    "1. Train your CharRNN on the Sherlock Holmes dataset provided. Include 3-5 samples generated from the model once you are reasonably confident in your model’s modelling abilities. Show how the temperature parameter affects the output. (Give some samples with low, medium, high temperatures). Try to get the best results possible from the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37aae1ba-d27b-4365-837d-68c032de2b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model train..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [00:21<00:00,  4.55it/s]\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 512\n",
    "embedding_size = 300\n",
    "seq_len = 100\n",
    "lr = 0.002\n",
    "num_epochs = 100\n",
    "epoch_size = 10  # one epoch is this # of examples\n",
    "out_seq_len = 200\n",
    "data_path = \"./data/sherlock.txt\"\n",
    "\n",
    "# code to initialize dataloader, model\n",
    "dataset = CharSeqDataloader(\n",
    "    filepath=data_path, seq_len=seq_len, examples_per_epoch=epoch_size\n",
    ")\n",
    "model = CharRNN(\n",
    "    n_chars=len(dataset.unique_chars),\n",
    "    embedding_size=embedding_size,\n",
    "    hidden_size=hidden_size,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Train the model\n",
    "progress['Sherlock_CharRNN'] = train(model, dataset, lr=lr, out_seq_len=out_seq_len, num_epochs=num_epochs,\n",
    "     sample_file='data/Sherlock_CharRNN.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe9920c-fc09-4fbd-b4d2-d2e80c442645",
   "metadata": {},
   "source": [
    "2. Train your CharRNN on the Shakespeare dataset provided. Do the same as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "888e24be-4c75-4825-b522-b4a6f1d83ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model train..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [00:21<00:00,  4.61it/s]\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 512\n",
    "embedding_size = 300\n",
    "seq_len = 100\n",
    "lr = 0.002\n",
    "num_epochs = 100\n",
    "epoch_size = 10  # one epoch is this # of examples\n",
    "out_seq_len = 200\n",
    "data_path = \"./data/shakespeare.txt\"\n",
    "\n",
    "# code to initialize dataloader, model\n",
    "dataset = CharSeqDataloader(\n",
    "    filepath=data_path, seq_len=seq_len, examples_per_epoch=epoch_size\n",
    ")\n",
    "model = CharRNN(\n",
    "    n_chars=len(dataset.unique_chars),\n",
    "    embedding_size=embedding_size,\n",
    "    hidden_size=hidden_size,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Train the model\n",
    "progress['Shakespeare_CharRNN'] = train(model, dataset, lr=lr, out_seq_len=out_seq_len, num_epochs=num_epochs,\n",
    "     sample_file='data/Shakespeare_CharRNN.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22879f45-2d1c-4680-b996-66cae4a4326b",
   "metadata": {},
   "source": [
    "**For the REPORT, please do the following:**\n",
    "1. Train your CharLSTM on the Sherlock Holmes dataset provided. Include 3-5 samples generated from the model once you are reasonably confident in your model’s modelling abilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b272661-048c-4de8-b30b-20239452c36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model train..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|█████████████████████████▌                | 61/100 [08:13<04:42,  7.25s/it]"
     ]
    }
   ],
   "source": [
    "hidden_size = 512\n",
    "embedding_size = 300\n",
    "seq_len = 100\n",
    "lr = 0.002\n",
    "num_epochs = 100\n",
    "epoch_size = 10  # one epoch is this # of examples\n",
    "out_seq_len = 200\n",
    "data_path = \"./data/sherlock.txt\"\n",
    "\n",
    "# code to initialize dataloader, model\n",
    "dataset = CharSeqDataloader(\n",
    "    filepath=data_path, seq_len=seq_len, examples_per_epoch=epoch_size\n",
    ")\n",
    "model = CharLSTM(\n",
    "    n_chars=len(dataset.unique_chars),\n",
    "    embedding_size=embedding_size,\n",
    "    hidden_size=hidden_size,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Train the model\n",
    "progress['Sherlock_CharLSTM'] = train(model, dataset, lr=lr, out_seq_len=out_seq_len, num_epochs=num_epochs,\n",
    "     sample_file='data/Sherlock_CharLSTM.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42a85c8-f41c-497d-863d-52c90709dc61",
   "metadata": {},
   "source": [
    "2. Train your CharLSTM on the Shakespeare dataset provided. Do the same as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12f2df2-8d06-4636-b5ef-78de774a7a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "embedding_size = 300\n",
    "seq_len = 100\n",
    "lr = 0.002\n",
    "num_epochs = 100\n",
    "epoch_size = 10  # one epoch is this # of examples\n",
    "out_seq_len = 200\n",
    "data_path = \"./data/shakespeare.txt\"\n",
    "\n",
    "# code to initialize dataloader, model\n",
    "dataset = CharSeqDataloader(\n",
    "    filepath=data_path, seq_len=seq_len, examples_per_epoch=epoch_size\n",
    ")\n",
    "model = CharLSTM(\n",
    "    n_chars=len(dataset.unique_chars),\n",
    "    embedding_size=embedding_size,\n",
    "    hidden_size=hidden_size,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Train the model\n",
    "progress['Shakespeare_CharLSTM'] = train(model, dataset, lr=lr, out_seq_len=out_seq_len, num_epochs=num_epochs,\n",
    "     sample_file='data/Shakespeare_CharLSTM.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74b2846-b7c3-4b3d-acb1-41d0c82c1e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data/progress.json', 'w') as f:\n",
    "    json.dump(progress, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5369c0d-dbe5-448b-8d6e-3d6ddaa51c26",
   "metadata": {},
   "source": [
    "3. Note some observations regarding training your CharRNN vs. CharLSTM. Is training faster or slower? How does the training loss compare? Graph the loss. Is the final model better or worse at language modeling, and in what way? Any specific strengths or weaknesses you can observe for each model?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
